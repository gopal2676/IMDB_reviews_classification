{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "typical-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Needed Libraries\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "therapeutic-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "imdb,info = tfds.load('imdb_reviews',with_info = True,as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interested-briefs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
     ]
    }
   ],
   "source": [
    "# Get the train and test data\n",
    "train_data = imdb['train']\n",
    "test_data = imdb['test']\n",
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "likely-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the tensors in our data to numpy arrays and futher to string in case of text\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "for s,l in train_data:\n",
    "    train_sentences.append(str(s.numpy()))\n",
    "    train_labels.append(l.numpy())\n",
    "for s,l in test_data:\n",
    "    test_sentences.append(str(s.numpy()))\n",
    "    test_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "similar-chair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"This was an absolutely terrible movie. Don\\'t be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie\\'s ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor\\'s like Christopher Walken\\'s good name. I could barely sit through it.\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See an example\n",
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adverse-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change labels to numpy array to be able to feed it to the model\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "correct-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-params and tokenizer Initialization \n",
    "vocab_size = 10000\n",
    "trunc_type = 'post'\n",
    "padd_type = 'post'\n",
    "embed_dims = 32\n",
    "max_len = 120\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size,oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index =  tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "express-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "limited-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all the examples of same length : truncate or pad if neccesary\n",
    "train_padded = pad_sequences(train_sequences,maxlen =max_len,truncating = trunc_type, padding = padd_type)\n",
    "test_padded = pad_sequences(test_sequences,maxlen =max_len,truncating = trunc_type, padding = padd_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "naughty-offset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 32)           320000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 321,089\n",
      "Trainable params: 321,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Embedding(vocab_size,embed_dims,input_length = max_len),\n",
    "                                   tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                                   tf.keras.layers.Dense(32,activation='relu'),\n",
    "                                   tf.keras.layers.Dense(1,activation='sigmoid')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "resident-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss= 'binary_crossentropy',optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beautiful-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 13s 15ms/step - loss: 0.5887 - accuracy: 0.6944 - val_loss: 0.3737 - val_accuracy: 0.8346\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.2874 - accuracy: 0.8845 - val_loss: 0.3824 - val_accuracy: 0.8297\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.2326 - accuracy: 0.9114 - val_loss: 0.4116 - val_accuracy: 0.8260\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.1928 - accuracy: 0.9312 - val_loss: 0.4572 - val_accuracy: 0.8126\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.1669 - accuracy: 0.9426 - val_loss: 0.4939 - val_accuracy: 0.8130\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.1508 - accuracy: 0.9478 - val_loss: 0.5477 - val_accuracy: 0.8048\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.1294 - accuracy: 0.9572 - val_loss: 0.5982 - val_accuracy: 0.8023\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.1141 - accuracy: 0.9649 - val_loss: 0.6726 - val_accuracy: 0.7901\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.1099 - accuracy: 0.9663 - val_loss: 0.7283 - val_accuracy: 0.7932\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.0943 - accuracy: 0.9721 - val_loss: 0.7906 - val_accuracy: 0.7907\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = model.fit(train_padded,train_labels,epochs=10,validation_data = (test_padded,test_labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "proper-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Let's get the embedding matrix(weights of the embedding layer)\n",
    "all_layers = model.layers\n",
    "needed_layer = all_layers[0]\n",
    "weight = needed_layer.get_weights()[0]\n",
    "print(weight.shape) # Shape will be dimension as of number of features(size of dictionary) and embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "administrative-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0973591   0.00079878  0.03082024  0.0144355   0.01587866 -0.02463027\n",
      " -0.04811297 -0.04403329 -0.0025988  -0.06331483 -0.01936501 -0.11508726\n",
      " -0.01364971 -0.04665851  0.02233699 -0.01682792  0.03840815 -0.02457084\n",
      "  0.02571667 -0.01190272  0.01988457  0.16575535 -0.06886932  0.01201383\n",
      "  0.05539486 -0.02649903  0.07671788  0.02958187 -0.01838448  0.028827\n",
      "  0.02292221  0.00339327]\n"
     ]
    }
   ],
   "source": [
    "print(weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cutting-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(index,word) for (word,index) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "vietnamese-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can project the data at - projector.tensorflow.org\n",
    "import io\n",
    "out_v = io.open('vecs.tsv','w',encoding='utf-8')\n",
    "out_m = io.open('meta.tsv','w',encoding='utf-8')\n",
    "for word in range(1,vocab_size):\n",
    "    curr_word = reverse_word_index[word]\n",
    "    embeddings = weight[word]\n",
    "    out_m.write(curr_word+\"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-framing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
